#!/usr/perl5/5.18.1/bin/perl

use feature ':5.18';
use strict;
use warnings;

# VERSION

package DateTime::Format::Stats;

use DateTime qw();

use DateTime::Format::Builder (
  # 2014 Nov  5 11:41:47
  parsers => {
    parse_datetime => [
      {
        strptime => '%Y %B %d %H:%M:%S',
      },
      {
        regex => qr/^(\d{4}) \s+ (\w{3}) \s+ (\d+) \s+
                     (\d+):(\d+):(\d+)$/x,
        params => [qw( year month day hour minute second )],
      },
      { 
        length => 8,
        regex  => qr/^(\d{2}):(\d{2}):(\d{2})$/x,
        params => [qw( hour minute second )],
        extra  => { time_zone => 'floating' },
        preprocess  => \&_add_in_fake_date,
      }
    ]
  }
);

sub _add_in_fake_date {
  my %args = @_;
  my ($date, $p) = @args{qw( input parsed )};
  # Yeah, the month and day have to be between 1 and 12
  @{$p}{qw( year month day )} = (0, 1, 1);
  return $date;
}


package Solaris::PerfParser;

# Enable 'static' variables
use feature    qw( state );

use Moose::Role;
use namespace::autoclean;
use IO::File                   qw();
#use DateTime::Format::Stats    qw();
use DateTime::Format::Strptime qw();
use DateTime::Set              qw();
use Fcntl                      qw(SEEK_SET);

requires '_build_dt_regex';
requires '_build_strptime_pattern';
requires '_parse_interval';

has 'datastream' => ( is       => 'rw',
                      isa      => 'IO::File',
                      required => 1,
                    );

# TODO: POD document
# If start/end are defined upon object construction, then we will seek
# the datastream until we find them
has 'start'          => ( is      => 'rw',
                          isa     => 'DateTime',
                          default => undef,
                        );
has 'end'            => ( is      => 'rw',
                          isa     => 'DateTime',
                          default => undef,
                        );
# Whether the two DateTimes above have been 'primed'; that is, whether
# they have had the year/month/day copied from the file we're parsing
# into these DateTimes.
has 'dt_primed'      => ( is      => 'rw',
                          isa     => 'Bool',
                          default => 0,
                        );

# TODO: POD document!
# If we need to collapse/coalesce the data, not necessarily keeping the
# timestamps once we've extracted the data from the proper time frame, then
# we'll need to set this to True.
# This is handy for DTrace stack traces when creating Flame Graphs from them
# after collapsing/coalescing them.
has 'strip_datetime' => ( is      => 'ro',
                          isa     => 'Bool',
                          default => 0,
                        );

# TODO: POD document
# This is something we'll set false if we don't want to further parse the
# data, such as DTrace stack traces, which need no further parsing other
# than collecting them from the right time range, then removing the
# timestamps for subsequent collapsing.
has 'interval_subparse' => ( is      => 'ro',
                             isa     => 'Bool',
                             default => 1,
                           );

has 'interval_data' => ( is      => 'rw',
                         isa     => 'ArrayRef[HashRef]',
                         default => sub { [] },
                       );

# Number of records discovered in the scan
has 'record_count' => ( is => 'rw', isa => 'Int', default => 0 );

# The size of each chunk read from the datastream
has 'read_chunk'   => ( is => 'ro', isa => 'Int', default => 1024 * 1024 );

# The Date/Time regex; its *existence* should be universal across all
# *stat and other data gathering applications like kernel stacks
has 'dt_regex' => (
  is      => 'ro',
  isa     => 'RegexpRef',
  lazy    => 1,
  builder => '_build_dt_regex',
);

has 'datetime_parser' => (
  is      => 'ro',
  isa     => 'Object',
  lazy    => 1,
  builder => '_build_datetime_parser',
);

# Regex used to pull individual records from the input, breaking them up into
# a datetime stamp, and the raw data (as $1 and $2)
has 'regex'    => ( is => 'rw', isa => 'RegexpRef',
                    lazy => 1, # Must be lazy because it depends on dt_regex
                    default => sub {
                                 my $self = shift;
                                 my $dt_regex = $self->dt_regex;
                               qr{(
                                    $dt_regex   # date-timestamp
                                    (?:.+?)     # all data after date-timestamp
                                  )
                                  # Up to, but not including, the next date/timestamp
                                  # (?= (?: $self->dt_regex | \z ) )
                                  (?= (?: $dt_regex ) )
                                 }smx;
                               },
                  );

# regex to use at EOF
has 'regex_eof' => ( is => 'rw', isa => 'RegexpRef',
                     lazy => 1, # Must be lazy because it depends on dt_regex
                     default => sub {
                                  my $self = shift;
                                  my $dt_regex = $self->dt_regex;
                                qr{(
                                     $dt_regex   # date-timestamp
                                     (?:.+?)     # all data after date-timestamp
                                   )
                                   # Up to, but not including, the next date/timestamp
                                   (?= (?: $dt_regex | \z ) )
                                  }smx;
                                },
                   );

sub scan {
  my ($self) = shift;

  my ($buf, $c);
  my ($dt_regex)  = $self->dt_regex;
  my ($regex)     = $self->regex;
  my ($regex_eof) = $self->regex_eof;
  my ($READ_SZ)   = $self->read_chunk;

  my $datetime_parser = $self->datetime_parser;

  my $i = 0;
  while ($self->datastream->read($buf,$READ_SZ)) {
    $c .= $buf;
    # Extract as many whole sections as possible, process, then
    # continue on
    my (@subs);
    # If we're at the end of the file, then we need to use a special case regex
    if ($self->datastream->eof) {
      @subs = $c =~ m{ $regex_eof }gsmx;
    } else {
      @subs = $c =~ m{ $regex }gsmx;
    }
    if (@subs) {
      my ($data,$dt_stamp,$coredata,$drops);
      for (my $i = 0; $i < scalar(@subs); $i++) {
        $data = $subs[$i];
        # Break out timestamp, parse into DateTime object
        ($dt_stamp) = $data =~ m/ ($dt_regex) /smx;
        chomp $dt_stamp;
        ($coredata = $data) =~ s/ $dt_regex //smx;
        # Rip out the time zone so parse_datetime will accept the $dtstamp
        $dt_stamp =~ s{\w+ \s+ (\d{4})$}{$1}x;
        # Of course, some like pgstat are different (the time zone comes last)
        $dt_stamp =~ s{(\d{4}) \s+ \w+$}{$1}x;
        my ($dt) = DateTime::Format::Stats->parse_datetime($dt_stamp);
        # TODO: Find a way to store the $coredata we need to examine eventually
        # push @{$dt_aref} = $dt;
      }
      # Delete what we've parsed so far...
      if ($self->datastream->eof) {
        $drops = $c =~ s{ $regex_eof }{}gsmx;
      } else {
        $drops = $c =~ s{ $regex }{}gsmx;
      }
      $self->record_count($self->record_count + $drops);
    }
  }
}

sub reset {
  my ($self) = shift;

  $self->record_count(0);
  $self->datastream->seek(0, SEEK_SET);
}

=head2 next

Pull all the records for the next time interval off of the datastream and
store them in the object's interval_data attribute.

if start and end DateTime's are defined, then make sure we're in the range
specified before returning anything.

=cut

sub next {
  my ($self) = shift;
  my ($buf);

  # Use a static/stateful variable, as between calls it's likely that you'll
  # have partially consumed/parsed data that you've read from the datastream
  state $c = '';

  my ($dt_regex)  = $self->dt_regex;
  my ($regex)     = $self->regex;
  my ($regex_eof) = $self->regex_eof;
  my ($READ_SZ)   = $self->read_chunk;
  # when we first pull these, they're only times, no year/month/day - we need
  # to populate them from the first datetime we read out of the file
  my ($start_dt)  = $self->start;
  my ($end_dt)    = $self->end;
  # Set to show you've primed the DateTimes from the file we're parsing
  my ($dt_primed) = $self->dt_primed;
  # If a time range has been specified, whether we've exhausted the range of
  # times of interest
  my ($time_range_exhausted);

  # If we have already parsed data, return it one interval at a
  # time
  # CONSTRAINT: When start_dt/end_dt defined, any data already parsed and ready
  #             for reading should be within the specified range
  if (scalar(@{$self->interval_data}) and (not $time_range_exhausted)) {
    return shift @{$self->interval_data};
  }

  # If the interval data is exhausted, try to extract more from the datastream
  do {
    $self->datastream->read($buf,$READ_SZ);
    $c .= $buf;
    # Extract as many whole sections as possible, process, then
    # continue on
    my (@subs);
    # If we're at the end of the file, then we need to use a special case regex
    if ($self->datastream->eof) {
      @subs = $c =~ m{ $regex_eof }gsmx;
    } else {
      @subs = $c =~ m{ $regex }gsmx;
    }
    if (@subs) {
      my ($data,$dt_stamp,$coredata,$esecs,$drops);
      for (my $i = 0; $i < scalar(@subs); $i++) {
        $data = $subs[$i];
        # Break out timestamp, parse into DateTime object
        ($dt_stamp) = $data =~ m/ ($dt_regex) /smx;
        chomp $dt_stamp;
        ($coredata = $data) =~ s/ $dt_regex //smx;
        # Rip out the time zone so parse_datetime will accept the $dtstamp
        $dt_stamp =~ s{\w+ \s+ (\d{4})$}{$1}x;
        # Of course, some like pgstat are different (the time zone comes last)
        $dt_stamp =~ s{(\d{4}) \s+ \w+$}{$1}x;
        my ($dt) = DateTime::Format::Stats->parse_datetime($dt_stamp);
        unless ($dt_primed) {
          my ($year,$month,$day) = ($dt->year, $dt->month, $dt->day);
          $start_dt->set_year($year);       $end_dt->set_year($year);
          $start_dt->set_month($month);     $end_dt->set_month($month);
          $start_dt->set_day($day);         $end_dt->set_day($day);
          $self->start($start_dt);   $self->end($end_dt);
          $dt_primed = $self->dt_primed(1);
        }
        # Exit the loop here if we've passed the end of the interesting time range
        if (defined($end_dt) and ($dt > $end_dt)) {
          $time_range_exhausted++;
          last;
        }
        # TODO: Parse the individual data sections into something we can print
        #       out directly in any format
        my $data;
        if ($self->interval_subparse) {
          my ($data) = $self->_parse_interval($coredata);
          $data->{datetime} = $dt;
        } else {
          $data = $coredata;
        }
        # Handle whether we're looking for a specific range or not
        if ($start_dt and $end_dt) {
          if (($start_dt <= $dt) and ($dt <= $end_dt)) {
            push @{$self->interval_data}, $data;
          }
        } else {
          push @{$self->interval_data}, $data;
        }
      }
      # Delete what we've parsed so far from our contents buffer...
      if ($self->datastream->eof) {
        $drops = $c =~ s{ $regex_eof }{}gsmx;
      } else {
        $drops = $c =~ s{ $regex }{}gsmx;
      }
      # Update the record count
      $self->record_count($self->record_count + $drops);
    }
  } until (scalar(@{$self->interval_data}) or $self->datastream->eof or
           $time_range_exhausted);

  if ( (not $time_range_exhausted) and scalar(@{$self->interval_data}) ) {
    return shift @{$self->interval_data};
  } else {
    return;  # undef
  }
}





package Solaris::PerfParser::DTraceStack;

use Moose;
use namespace::autoclean;

with 'Solaris::PerfParser';

# Result of DTrace's printf("%Y\n",walltimestamp)
# EXAMPLE: 2014 Nov  6 14:00:00
sub _build_dt_regex {
  return qr{^
            (?: \d{4} \s+        # year
                (?:Jan|Feb|Mar|Apr|May|Jun|
                   Jul|Aug|Sep|Oct|Nov|Dec
                ) \s+
                \d+ \s+          # day of month
                \d+:\d+:\d+      # HH:MM:DD  (24 hour clock)
            )
           }smx;
}

sub _build_strptime_pattern {
  return "%A %B %d %T %Y";
}

=head2 _parse_interval

Parse data for a single time interval (unnecessary for a stack trace)

=cut

sub _parse_interval {
  my ($self,$data) = @_;

  return; #undef
}


__PACKAGE__->meta->make_immutable;






package main;

use feature ':5.18';
use strict;
use warnings;

use feature qw(say);

use IO::File                   qw();
use DateTime                   qw();
use Getopt::Long               qw();
use Fcntl                      qw(SEEK_SET);
use File::Temp                 qw( :seekable );
#use Solaris::FlameGraph       qw();
use DateTime                   qw();
use DateTime::TimeZone         qw();
use DateTime::Format::Strptime qw();
use LWP::UserAgent             qw();
use HTTP::Request::Common      qw( POST PUT );
use FindBin                    qw();
use URI::Escape                qw();
use IO::Uncompress::Bunzip2    qw();
use File::MMagic               qw();
use Carp                       qw(croak);
use File::stat                 qw();

use lib "$FindBin::Bin/../lib";
# use Perf::Schema qw();

use vars qw($HAS_PROGRESS_BAR);

BEGIN {
  eval {
    require Term::ProgressBar;
    $HAS_PROGRESS_BAR = 1;
  };
}

#
# Open specified stack trace file, and extract data in between datetime stamp 1
# and 2.
#
# Submit the resulting file up to the REST server for processing via PUT
#
# Receive back data containing info on what URL you can see the resulting
# flamegraph from
#
#

my $file;
my ($start,$end,$hostname,$time_zone,$start_dt,$end_dt,$description,
    $server,$port);

Getopt::Long::GetOptions(
  'file=s'       => \$file,
  'start=s'      => \$start,
  'end=s'        => \$end,
  'hostname=s'   => \$hostname,
  'time_zone=s'  => \$time_zone,
  'desc=s'       => \$description,
  'port=s'       => \$port,
  'server=s'     => \$server,
) or die <<USAGE_END;

USAGE: $0 [options]\n
    --file  <file>          # File containing stack trace
    --start [ HH:MM:SS ]    # Starting timestamp
    --end   [ HH:MM:SS ]    # Ending   timestamp
  [ --hostname <hostname> ] # Hostname override
  [ --time_zone 'TZname' ]  # Time Zone override
  [ --desc "description"  ] # Description, which will be used in flamegraph title
  [ --server     <server> ] # Destination web server
  [ --port         <port> ] # Port on destination web server
USAGE_END

my $tzname = $time_zone ||= DateTime::TimeZone->new( name => 'local' )->name;
$hostname = $hostname ||= qx{/bin/hostname};
chomp($hostname);

say "Collecting kernel stack trace for $hostname, in [$tzname] time zone";

#
# NOTE: We're presuming that we're parsing the log in question on the same
#       machine where that log was created.  Thus we're using the local time
#       zone here, so we can reliably convert to UTC and back later as we
#       store the data in a database.
$start_dt =
  DateTime::Format::Stats->parse_datetime($start)->set_time_zone($tzname);
$end_dt   =
  DateTime::Format::Stats->parse_datetime($end)->set_time_zone($tzname);

my $fh   = IO::File->new($file, "<") or die "Unable to open $file: $!";
my $mm   = File::MMagic->new();
my $mime_type = $mm->checktype_filehandle($fh);

# Handle bzip2 compressed files
if ($mime_type eq 'application/x-bzip2') {
  $fh = IO::Uncompress::Bunzip2->new($file) or
    croak "Unable top open bzip2 file: $file";
}

my $p = Solaris::PerfParser::DTraceStack->new(
          datastream => $fh, interval_subparse => 0, strip_datetime => 1,
          start => $start_dt, end => $end_dt,
        );

my $tfh = File::Temp->new() or croak("Unable to open temporary file");

while (my $d = $p->next) {
  $tfh->print($d);
}
# Make sure we flush the contents to the temp file, so the stat below won't
# think the file is empty when it's not
$tfh->flush;
$tfh->close;

# If we didn't store anything in the temporary file, then something is wrong,
# abort
my $stat = File::stat::stat($tfh->filename);
if ( $stat->size == 0 ) {
  croak "Nothing made it into the temporary file";
}


# While operational, the Solaris::PerfParser::DTraceStack object has properly
# filled in the date for the start/end times we plucked out, so extract them.
$start_dt = $p->start;
$end_dt   = $p->end;

# Seek to beginning of file to allow sending it somewhere
$tfh->seek( 0, SEEK_SET );

http_upload($tfh->filename,$hostname,$start_dt->epoch,$end_dt->epoch,
            $tzname,$server,$port,$description);

exit 0;

sub http_upload {
  my ($filename,$hostname,$begin_epoch,$end_epoch,$tz,$server,$port,$description) = @_;

  $HTTP::Request::Common::DYNAMIC_FILE_UPLOAD = 1;

  my ($ua) = LWP::UserAgent->new;
  
  $server = $server || 'localhost';
  $port   = $port || 3000;
  $tz = URI::Escape::uri_escape($tz);
  my $req = POST "http://$server:$port/flamegraph/upload_stack/$hostname/$begin_epoch/$end_epoch/$tz",
    Content_Type => 'form-data',
    Content      => {
      description => $description,
      upload      => [ $filename ],
    };
#    ':read_size_hint' => 131072,

  my $file_size = $req->header('Content_Length');
  print "Starting Upload of file size $file_size...\n";

  my $reader = &create_content_reader($req->content(),
                                      $req->header('Content_Length'));
  $req->content($reader);

  my $resp = $ua->request($req);
  die "error while uploading $filename: ", $resp->message if $resp->is_error;

  say "\n\nPlease point your browser at this URL:\n" . $resp->content . "\n";

  sub create_content_reader {
    my $gen = shift;
    my $len = shift;

    if ($HAS_PROGRESS_BAR) {
      my $progress = Term::ProgressBar->new({ name => "Upload", count => $len,
          term_width => 80 });

      $progress->minor(0);

      my $size = 0;
      my $next_update = 0;

      return sub {
        my $chunk = $gen->();
        $size += length($chunk) if $chunk;
        $next_update = $progress->update($size) if $size >= $next_update;
        return $chunk;
      }
    } else {
      return sub {
        return $gen->();
      }
    }
  }

}


